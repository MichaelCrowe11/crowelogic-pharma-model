# CroweLogic-Pharma Pro (70b) GPU-Accelerated Dockerfile
# Uses NVIDIA CUDA runtime to enable GPU inference for the 70B model

FROM nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    OLLAMA_HOST=0.0.0.0:11434 \
    OLLAMA_MAX_LOADED_MODELS=1 \
    OLLAMA_NUM_PARALLEL=1 \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Install system dependencies and Python
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-venv \
    curl \
    git \
    build-essential \
    ca-certificates \
    && ln -s /usr/bin/python3 /usr/local/bin/python \
    && ln -s /usr/bin/pip3 /usr/local/bin/pip \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy dependency list first for caching
COPY requirements.txt ./requirements.txt

# Install Python dependencies
RUN pip install --upgrade pip && \
    pip install -r requirements.txt

# Copy application artifacts
COPY models/ ./models/
COPY training_data/ ./training_data/
COPY scripts/ ./scripts/

# Install Ollama with GPU support
RUN curl -fsSL https://ollama.com/install.sh | sh

# Expose service ports
EXPOSE 11434 8000

# GPU-aware entrypoint for the 70B model
RUN echo '#!/bin/bash\n\
set -e\n\
echo "========================================"\n\
echo "CroweLogic-Pharma Pro (70b) GPU Start"\n\
echo "========================================"\n\
echo "Starting Ollama service with GPU access..."\n\
ollama serve &\n\
OLLAMA_PID=$!\n\
echo "Ollama PID: $OLLAMA_PID"\n\
\n\
echo "Verifying GPU availability..."\n\
if command -v nvidia-smi >/dev/null 2>&1; then\n\
  nvidia-smi\n\
else\n\
  echo "⚠️  nvidia-smi not found; ensure drivers are mounted in host."\n\
fi\n\
\n\
echo "Waiting for Ollama to initialize..."\n\
sleep 12\n\
\n\
if ! ollama list | grep -q "CroweLogic-Pharma-Pro"; then\n\
  echo "Pulling llama3.1:70b base model (GPU optimized)..."\n\
  ollama pull llama3.1:70b\n\
  echo "Building CroweLogic-Pharma-Pro model..."\n\
  ollama create CroweLogic-Pharma-Pro:latest -f /app/models/CroweLogicPharmaModelfile-70b\n\
  echo "✅ Model ready for GPU inference."\n\
else\n\
  echo "✅ CroweLogic-Pharma-Pro model already materialized."\n\
fi\n\
\n\
echo "========================================"\n\
echo "CroweLogic-Pharma Pro (70b) Online"\n\
echo "========================================"\n\
echo "GPU-accelerated endpoint: http://<load-balancer>:11434"\n\
echo "Model: CroweLogic-Pharma-Pro:latest"\n\
\n\
# Keep container running while Ollama serves requests\n\
tail -f /dev/null\n' > /app/entrypoint-gpu.sh && chmod +x /app/entrypoint-gpu.sh

ENTRYPOINT ["/app/entrypoint-gpu.sh"]
